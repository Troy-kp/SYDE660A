#!/usr/bin/python3
# coding=utf-8
# author troy

import json
import time
import re
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from bs4 import BeautifulSoup
import logging

class AcademicCalendarScraper:
    """
    é«˜çº§å­¦æœ¯æ—¥å†çˆ¬å–å™¨
    ä¸“é—¨å¤„ç†æ»‘é“å¢å¤§å­¦JavaScriptæ¸²æŸ“çš„å­¦æœ¯æ—¥å†ç½‘ç«™
    """
    
    def __init__(self, headless=True):
        self.base_url = "https://uwaterloo.ca/academic-calendar/graduate-studies/catalog#/programs"
        self.headless = headless
        self.driver = None
        self.wait = None
        
        # è®¾ç½®æ—¥å¿—
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
        # æ•°æ®å­˜å‚¨
        self.programs_data = []
        
    def setup_driver(self):
        """è®¾ç½®Chrome WebDriver"""
        try:
            chrome_options = Options()
            if self.headless:
                chrome_options.add_argument("--headless")
            chrome_options.add_argument("--no-sandbox")
            chrome_options.add_argument("--disable-dev-shm-usage")
            chrome_options.add_argument("--disable-gpu")
            chrome_options.add_argument("--window-size=1920,1080")
            chrome_options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36")
            
            self.driver = webdriver.Chrome(options=chrome_options)
            self.wait = WebDriverWait(self.driver, 20)
            
            self.logger.info("âœ… Chrome WebDriver è®¾ç½®æˆåŠŸ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ WebDriver è®¾ç½®å¤±è´¥: {e}")
            return False
    
    def wait_for_page_load(self, timeout=20):
        """ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½"""
        try:
            # ç­‰å¾…é¡µé¢åŸºæœ¬å…ƒç´ åŠ è½½
            self.wait.until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            
            # ç­‰å¾…JavaScriptæ¸²æŸ“å®Œæˆ
            time.sleep(3)
            
            # æ£€æŸ¥æ˜¯å¦è¿˜åœ¨loadingçŠ¶æ€
            loading_indicators = ["Loading", "loading", "spinner"]
            max_attempts = 10
            
            for attempt in range(max_attempts):
                page_text = self.driver.page_source.lower()
                if not any(indicator.lower() in page_text for indicator in loading_indicators):
                    break
                time.sleep(2)
                
            self.logger.info("âœ… é¡µé¢åŠ è½½å®Œæˆ")
            return True
            
        except TimeoutException:
            self.logger.warning("âš ï¸ é¡µé¢åŠ è½½è¶…æ—¶ï¼Œä½†ç»§ç»­æ‰§è¡Œ")
            return False
    
    def extract_programs_list(self):
        """æå–æ‰€æœ‰ä¸“ä¸šé¡¹ç›®çš„åŸºæœ¬ä¿¡æ¯"""
        try:
            self.logger.info("ğŸ” å¼€å§‹æå–ä¸“ä¸šé¡¹ç›®åˆ—è¡¨...")
            
            # ç­‰å¾…é¡µé¢å…ƒç´ åŠ è½½
            time.sleep(5)
            
            # è·å–é¡µé¢æºä»£ç å¹¶è§£æ
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            
            # æŸ¥æ‰¾ä¸“ä¸šé¡¹ç›®é“¾æ¥çš„å¤šç§å¯èƒ½é€‰æ‹©å™¨
            program_selectors = [
                'a[href*="/programs/"]',
                'a[href*="graduate"]',
                '.program-link',
                '.program-item a',
                '[data-program]',
                'a[href*="masc"]',
                'a[href*="meng"]', 
                'a[href*="phd"]'
            ]
            
            programs_found = set()
            
            for selector in program_selectors:
                elements = soup.select(selector)
                for element in elements:
                    href = element.get('href', '')
                    text = element.get_text(strip=True)
                    
                    if href and text and len(text) > 5:
                        # æ„å»ºå®Œæ•´URL
                        if href.startswith('/'):
                            full_url = f"https://uwaterloo.ca{href}"
                        elif href.startswith('http'):
                            full_url = href
                        else:
                            continue
                            
                        # è¿‡æ»¤ç›¸å…³çš„ç ”ç©¶ç”Ÿé¡¹ç›®
                        if any(keyword in text.lower() for keyword in ['master', 'phd', 'doctorate', 'graduate', 'msc', 'masc', 'meng']):
                            programs_found.add((text, full_url))
            
            # è½¬æ¢ä¸ºåˆ—è¡¨å¹¶æ·»åŠ åŸºæœ¬ä¿¡æ¯
            for program_name, program_url in programs_found:
                program_info = {
                    'program_name': program_name,
                    'program_url': program_url,
                    'discovery_timestamp': datetime.now().isoformat(),
                    'source': 'academic_calendar'
                }
                self.programs_data.append(program_info)
            
            self.logger.info(f"âœ… å‘ç° {len(programs_found)} ä¸ªä¸“ä¸šé¡¹ç›®")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ æå–ä¸“ä¸šåˆ—è¡¨å¤±è´¥: {e}")
            return False
    
    def scrape_program_details(self, program_info):
        """çˆ¬å–å•ä¸ªä¸“ä¸šçš„è¯¦ç»†ä¿¡æ¯"""
        try:
            program_name = program_info['program_name']
            program_url = program_info['program_url']
            
            self.logger.info(f"ğŸ“š çˆ¬å–ä¸“ä¸šè¯¦æƒ…: {program_name}")
            
            # è®¿é—®ä¸“ä¸šè¯¦æƒ…é¡µé¢
            self.driver.get(program_url)
            self.wait_for_page_load()
            
            # è§£æé¡µé¢å†…å®¹
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            
            # æå–è¯¦ç»†ä¿¡æ¯
            details = {
                'program_name': program_name,
                'program_url': program_url,
                'degree_type': self._extract_degree_type(soup, program_name),
                'academic_level': self._extract_academic_level(soup, program_name),
                'duration': self._extract_duration(soup),
                'admission_requirements': self._extract_admission_requirements(soup),
                'course_requirements': self._extract_course_requirements(soup),
                'specializations': self._extract_specializations(soup),
                'certificates': self._extract_certificates(soup),
                'degree_requirements': self._extract_degree_requirements(soup),
                'research_areas': self._extract_research_areas(soup),
                'faculty_info': self._extract_faculty_info(soup),
                'application_info': self._extract_application_info(soup),
                'funding_info': self._extract_funding_info(soup),
                'scraped_timestamp': datetime.now().isoformat()
            }
            
            return details
            
        except Exception as e:
            self.logger.error(f"âŒ çˆ¬å– {program_name} è¯¦æƒ…å¤±è´¥: {e}")
            return None
    
    def _extract_degree_type(self, soup, program_name):
        """æå–å­¦ä½ç±»å‹"""
        degree_patterns = {
            'MASc': r'master.*applied.*science|masc',
            'MEng': r'master.*engineering|meng',
            'MSc': r'master.*science|msc',
            'PhD': r'phd|doctorate|doctor',
            'MArch': r'master.*architecture',
            'MMath': r'master.*mathematics'
        }
        
        text = soup.get_text().lower()
        program_name_lower = program_name.lower()
        
        for degree, pattern in degree_patterns.items():
            if re.search(pattern, text) or re.search(pattern, program_name_lower):
                return degree
        
        return 'Unknown'
    
    def _extract_academic_level(self, soup, program_name):
        """æå–å­¦æœ¯å±‚æ¬¡"""
        text = soup.get_text().lower()
        if 'phd' in text or 'doctorate' in text:
            return 'Doctoral'
        elif 'master' in text or any(deg in text for deg in ['masc', 'meng', 'msc']):
            return 'Master'
        return 'Unknown'
    
    def _extract_duration(self, soup):
        """æå–é¡¹ç›®æ—¶é•¿"""
        text = soup.get_text()
        
        # æŸ¥æ‰¾æ—¶é•¿æ¨¡å¼
        duration_patterns = [
            r'(\d+)\s*years?',
            r'(\d+)\s*terms?',
            r'(\d+)\s*months?',
            r'(\d+)\s*semesters?'
        ]
        
        for pattern in duration_patterns:
            match = re.search(pattern, text, re.I)
            if match:
                return {
                    'value': int(match.group(1)),
                    'unit': match.group(0).split()[-1].lower(),
                    'raw_text': match.group(0)
                }
        
        return None
    
    def _extract_admission_requirements(self, soup):
        """æå–å…¥å­¦è¦æ±‚"""
        requirements = []
        
        # æŸ¥æ‰¾åŒ…å«å…¥å­¦è¦æ±‚çš„ç« èŠ‚
        admission_keywords = ['admission', 'requirement', 'prerequisite', 'eligibility']
        
        for keyword in admission_keywords:
            sections = soup.find_all(text=re.compile(keyword, re.I))
            for section in sections[:3]:  # é™åˆ¶ç»“æœæ•°é‡
                parent = section.parent if hasattr(section, 'parent') else None
                if parent:
                    # è·å–çˆ¶çº§å…ƒç´ çš„æ›´å¤§èŒƒå›´æ–‡æœ¬
                    for ancestor in [parent, parent.parent, parent.parent.parent if parent.parent else None]:
                        if ancestor and ancestor.name:
                            text = ancestor.get_text(strip=True)
                            if len(text) > 50 and len(text) < 1000:
                                requirements.append({
                                    'type': keyword,
                                    'content': text[:500],  # é™åˆ¶é•¿åº¦
                                    'source_tag': ancestor.name
                                })
                                break
        
        return requirements[:5]  # æœ€å¤šè¿”å›5ä¸ªè¦æ±‚
    
    def _extract_course_requirements(self, soup):
        """æå–è¯¾ç¨‹è¦æ±‚"""
        requirements = []
        course_pattern = r'\b[A-Z]{2,6}\s*\d{3}[A-Z]?\b'
        
        # æŸ¥æ‰¾åŒ…å«è¯¾ç¨‹ä»£ç çš„æ–‡æœ¬
        text_elements = soup.find_all(['p', 'div', 'li', 'td'])
        
        for element in text_elements:
            text = element.get_text()
            courses = re.findall(course_pattern, text)
            
            if courses and len(text) > 20:
                # åˆ¤æ–­è¦æ±‚ç±»å‹
                req_type = 'general'
                lower_text = text.lower()
                
                if any(word in lower_text for word in ['core', 'required', 'mandatory', 'must']):
                    req_type = 'core'
                elif any(word in lower_text for word in ['elective', 'optional', 'choose', 'select']):
                    req_type = 'elective'
                elif any(word in lower_text for word in ['credit', 'unit']):
                    req_type = 'credit'
                
                requirements.append({
                    'type': req_type,
                    'courses': list(set(courses)),  # å»é‡
                    'description': text[:300],  # é™åˆ¶æè¿°é•¿åº¦
                    'course_count': len(set(courses))
                })
        
        return requirements[:10]  # æœ€å¤šè¿”å›10ä¸ªè¦æ±‚
    
    def _extract_specializations(self, soup):
        """æå–ä¸“ä¸šæ–¹å‘ä¿¡æ¯"""
        specializations = []
        spec_keywords = ['specialization', 'concentration', 'track', 'area', 'option', 'stream']
        
        for keyword in spec_keywords:
            elements = soup.find_all(text=re.compile(keyword, re.I))
            for element in elements[:5]:
                parent = element.parent if hasattr(element, 'parent') else None
                if parent:
                    text = parent.get_text(strip=True)
                    if 20 < len(text) < 500:
                        specializations.append({
                            'name': text[:100],
                            'type': keyword,
                            'description': text,
                            'keyword_matched': keyword
                        })
        
        return specializations
    
    def _extract_certificates(self, soup):
        """æå–è¯ä¹¦ä¿¡æ¯"""
        certificates = []
        cert_keywords = ['certificate', 'diploma', 'certification']
        
        for keyword in cert_keywords:
            elements = soup.find_all(text=re.compile(keyword, re.I))
            for element in elements[:3]:
                parent = element.parent if hasattr(element, 'parent') else None
                if parent:
                    text = parent.get_text(strip=True)
                    if 20 < len(text) < 300:
                        certificates.append({
                            'name': text[:100],
                            'description': text,
                            'type': keyword
                        })
        
        return certificates
    
    def _extract_degree_requirements(self, soup):
        """æå–å­¦ä½è¦æ±‚"""
        text = soup.get_text()
        requirements = {}
        
        # å­¦åˆ†è¦æ±‚
        credit_patterns = [
            r'(\d+)\s*credit',
            r'(\d+)\s*units?',
            r'(\d+)\s*course'
        ]
        
        for pattern in credit_patterns:
            match = re.search(pattern, text, re.I)
            if match:
                requirements['credits'] = {
                    'value': int(match.group(1)),
                    'description': match.group(0)
                }
                break
        
        # GPAè¦æ±‚
        gpa_match = re.search(r'(\d+\.?\d*)\s*(gpa|average|grade)', text, re.I)
        if gpa_match:
            requirements['gpa'] = {
                'value': float(gpa_match.group(1)),
                'description': gpa_match.group(0)
            }
        
        # è®ºæ–‡è¦æ±‚
        if re.search(r'thesis|dissertation|research', text, re.I):
            requirements['thesis_required'] = True
        
        return requirements
    
    def _extract_research_areas(self, soup):
        """æå–ç ”ç©¶é¢†åŸŸ"""
        research_areas = []
        research_keywords = ['research', 'area', 'field', 'topic']
        
        for keyword in research_keywords:
            sections = soup.find_all(text=re.compile(keyword, re.I))
            for section in sections[:3]:
                parent = section.parent if hasattr(section, 'parent') else None
                if parent:
                    text = parent.get_text(strip=True)
                    if 30 < len(text) < 400:
                        research_areas.append({
                            'area': text[:200],
                            'description': text,
                            'keyword': keyword
                        })
        
        return research_areas
    
    def _extract_faculty_info(self, soup):
        """æå–å­¦é™¢ä¿¡æ¯"""
        faculty_info = {}
        
        # æŸ¥æ‰¾å­¦é™¢åç§°
        faculty_patterns = [
            r'faculty of (\w+)',
            r'school of (\w+)',
            r'department of ([\w\s]+)'
        ]
        
        text = soup.get_text()
        for pattern in faculty_patterns:
            match = re.search(pattern, text, re.I)
            if match:
                faculty_info['faculty'] = match.group(1).title()
                break
        
        return faculty_info
    
    def _extract_application_info(self, soup):
        """æå–ç”³è¯·ä¿¡æ¯"""
        app_info = {}
        
        # æŸ¥æ‰¾ç”³è¯·æˆªæ­¢æ—¥æœŸ
        date_patterns = [
            r'deadline.*?(\w+\s+\d{1,2})',
            r'application.*?due.*?(\w+\s+\d{1,2})',
            r'submit.*?by.*?(\w+\s+\d{1,2})'
        ]
        
        text = soup.get_text()
        for pattern in date_patterns:
            match = re.search(pattern, text, re.I)
            if match:
                app_info['deadline'] = match.group(1)
                break
        
        return app_info
    
    def _extract_funding_info(self, soup):
        """æå–èµ„åŠ©ä¿¡æ¯"""
        funding_info = []
        funding_keywords = ['funding', 'scholarship', 'fellowship', 'assistantship', 'stipend']
        
        for keyword in funding_keywords:
            elements = soup.find_all(text=re.compile(keyword, re.I))
            for element in elements[:2]:
                parent = element.parent if hasattr(element, 'parent') else None
                if parent:
                    text = parent.get_text(strip=True)
                    if 20 < len(text) < 300:
                        funding_info.append({
                            'type': keyword,
                            'description': text[:200]
                        })
        
        return funding_info
    
    def save_data(self, filename):
        """ä¿å­˜çˆ¬å–çš„æ•°æ®"""
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(self.programs_data, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"âœ… æ•°æ®å·²ä¿å­˜åˆ°: {filename}")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ä¿å­˜æ•°æ®å¤±è´¥: {e}")
            return False
    
    def run_full_scrape(self, max_programs=None):
        """è¿è¡Œå®Œæ•´çš„çˆ¬å–æµç¨‹"""
        try:
            self.logger.info("ğŸš€ å¼€å§‹å­¦æœ¯æ—¥å†çˆ¬å–æµç¨‹")
            
            # è®¾ç½®WebDriver
            if not self.setup_driver():
                return False
            
            # è®¿é—®ä¸»é¡µé¢
            self.logger.info(f"ğŸ“¡ è®¿é—®ä¸»é¡µé¢: {self.base_url}")
            self.driver.get(self.base_url)
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            self.wait_for_page_load()
            
            # æå–ä¸“ä¸šåˆ—è¡¨
            if not self.extract_programs_list():
                return False
            
            # çˆ¬å–è¯¦ç»†ä¿¡æ¯
            detailed_programs = []
            total_programs = len(self.programs_data)
            max_programs = max_programs or total_programs
            
            self.logger.info(f"ğŸ“š å¼€å§‹çˆ¬å– {min(max_programs, total_programs)} ä¸ªä¸“ä¸šçš„è¯¦ç»†ä¿¡æ¯")
            
            for i, program_info in enumerate(self.programs_data[:max_programs]):
                try:
                    details = self.scrape_program_details(program_info)
                    if details:
                        detailed_programs.append(details)
                    
                    # è¿›åº¦æ˜¾ç¤º
                    self.logger.info(f"è¿›åº¦: {i+1}/{min(max_programs, total_programs)}")
                    
                    # ç¤¼è²Œæ€§å»¶è¿Ÿ
                    time.sleep(2)
                    
                except Exception as e:
                    self.logger.error(f"çˆ¬å–ç¬¬ {i+1} ä¸ªä¸“ä¸šå¤±è´¥: {e}")
            
            # æ›´æ–°æ•°æ®
            self.programs_data = detailed_programs
            
            # ä¿å­˜æ•°æ®
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"academic_calendar_programs_{timestamp}.json"
            self.save_data(filename)
            
            self.logger.info(f"ğŸ‰ çˆ¬å–å®Œæˆï¼æˆåŠŸçˆ¬å– {len(detailed_programs)} ä¸ªä¸“ä¸š")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ çˆ¬å–æµç¨‹å¤±è´¥: {e}")
            return False
            
        finally:
            if self.driver:
                self.driver.quit()
                self.logger.info("ğŸ”š WebDriver å·²å…³é—­")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ“ æ»‘é“å¢å¤§å­¦å­¦æœ¯æ—¥å†é«˜çº§çˆ¬å–å™¨")
    print("=" * 60)
    
    # åˆ›å»ºçˆ¬å–å™¨
    scraper = AcademicCalendarScraper(headless=True)
    
    # è¿è¡Œçˆ¬å–
    success = scraper.run_full_scrape(max_programs=10)  # å…ˆæµ‹è¯•10ä¸ªä¸“ä¸š
    
    if success:
        print("âœ… çˆ¬å–æˆåŠŸå®Œæˆï¼")
    else:
        print("âŒ çˆ¬å–å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—ä¿¡æ¯")

if __name__ == "__main__":
    main() 