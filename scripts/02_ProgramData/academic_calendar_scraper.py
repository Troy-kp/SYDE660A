#!/usr/bin/env python3
"""
Academic Calendarç›´æ¥è®¿é—®å·¥å…·
é’ˆå¯¹ https://uwaterloo.ca/academic-calendar/graduate-studies/catalog#/programs?isPrint=true
"""

import requests
import time
import json
import re
from datetime import datetime
from bs4 import BeautifulSoup

class AcademicCalendarScraper:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        
        self.base_url = "https://uwaterloo.ca/academic-calendar/graduate-studies/catalog"
        self.print_url = "https://uwaterloo.ca/academic-calendar/graduate-studies/catalog#/programs?isPrint=true"
    
    def access_academic_calendar(self):
        """è®¿é—®Academic Calendaré¡µé¢"""
        print("ğŸŒ è®¿é—®Academic Calendaré¡µé¢...")
        print(f"URL: {self.print_url}")
        
        try:
            response = self.session.get(self.print_url, timeout=30)
            response.raise_for_status()
            
            print(f"âœ… è®¿é—®æˆåŠŸï¼çŠ¶æ€ç : {response.status_code}")
            print(f"ğŸ“„ å“åº”é•¿åº¦: {len(response.text):,} å­—ç¬¦")
            
            return response.text
            
        except Exception as e:
            print(f"âŒ è®¿é—®å¤±è´¥: {e}")
            return None
    
    def analyze_page_structure(self, html_content):
        """åˆ†æé¡µé¢ç»“æ„"""
        print("\nğŸ” åˆ†æé¡µé¢ç»“æ„...")
        
        soup = BeautifulSoup(html_content, 'html.parser')
        
        analysis = {
            'title': soup.title.text if soup.title else 'No title',
            'has_javascript_loading': 'JavaScript must be enabled' in html_content,
            'has_angular_app': 'ng-app' in html_content or 'angular' in html_content.lower(),
            'scripts': len(soup.find_all('script')),
            'meta_tags': len(soup.find_all('meta')),
            'divs_with_ng': len(soup.find_all('div', attrs={'ng-app': True})),
            'potential_api_calls': []
        }
        
        # æŸ¥æ‰¾å¯èƒ½çš„APIè°ƒç”¨
        script_tags = soup.find_all('script')
        for script in script_tags:
            if script.string:
                # æŸ¥æ‰¾API URLæ¨¡å¼
                api_patterns = [
                    r'api/[^"\']+',
                    r'/catalog/[^"\']+',
                    r'\.json[^"\']*',
                    r'programs[^"\']*'
                ]
                
                for pattern in api_patterns:
                    matches = re.findall(pattern, script.string)
                    analysis['potential_api_calls'].extend(matches)
        
        # æŸ¥æ‰¾æ•°æ®å®¹å™¨
        data_containers = soup.find_all(['div', 'section', 'main'], 
                                       attrs={'class': re.compile(r'(content|programs|catalog|data)')})
        analysis['data_containers'] = len(data_containers)
        
        return analysis
    
    def try_alternative_urls(self):
        """å°è¯•è®¿é—®å¯èƒ½çš„æ›¿ä»£URL"""
        print("\nğŸ”— å°è¯•è®¿é—®æ›¿ä»£URL...")
        
        alternative_urls = [
            "https://uwaterloo.ca/academic-calendar/graduate-studies/catalog",
            "https://uwaterloo.ca/academic-calendar/graduate-studies/programs",
            "https://uwaterloo.ca/academic-calendar/graduate-studies/",
            "https://uwaterloo.ca/graduate-studies-academic-calendar/",
            "https://uwaterloo.ca/academic-calendar/graduate-studies/catalog/programs",
            f"{self.print_url}&format=json",
            f"{self.print_url}&print=1"
        ]
        
        results = {}
        
        for url in alternative_urls:
            print(f"  å°è¯•: {url}")
            try:
                response = self.session.get(url, timeout=15)
                results[url] = {
                    'status_code': response.status_code,
                    'content_length': len(response.text),
                    'content_type': response.headers.get('Content-Type', 'Unknown'),
                    'has_programs': 'program' in response.text.lower(),
                    'has_json': response.headers.get('Content-Type', '').startswith('application/json')
                }
                print(f"    âœ… {response.status_code} - {len(response.text):,} chars")
                
                # å¦‚æœæ‰¾åˆ°JSONæˆ–ç¨‹åºå†…å®¹ï¼Œä¿å­˜æ ·æœ¬
                if results[url]['has_json'] or results[url]['has_programs']:
                    sample_file = f"sample_content_{url.split('/')[-1].replace('?', '_').replace('=', '_').replace('#', '_')}.txt"
                    with open(sample_file, 'w', encoding='utf-8') as f:
                        f.write(response.text[:5000])  # ä¿å­˜å‰5000å­—ç¬¦
                    print(f"    ğŸ’¾ æ ·æœ¬ä¿å­˜åˆ°: {sample_file}")
                
            except Exception as e:
                results[url] = {'error': str(e)}
                print(f"    âŒ å¤±è´¥: {e}")
        
        return results
    
    def extract_javascript_data(self, html_content):
        """å°è¯•ä»JavaScriptä¸­æå–æ•°æ®"""
        print("\nğŸ’» åˆ†æJavaScriptå†…å®¹...")
        
        soup = BeautifulSoup(html_content, 'html.parser')
        js_data = {
            'script_sources': [],
            'inline_scripts': [],
            'potential_data': []
        }
        
        # æå–æ‰€æœ‰scriptæ ‡ç­¾
        scripts = soup.find_all('script')
        
        for script in scripts:
            if script.get('src'):
                js_data['script_sources'].append(script.get('src'))
            elif script.string:
                js_data['inline_scripts'].append(script.string[:500])  # å‰500å­—ç¬¦
                
                # æŸ¥æ‰¾å¯èƒ½çš„æ•°æ®ç»“æ„
                if 'programs' in script.string.lower():
                    js_data['potential_data'].append({
                        'type': 'programs_related',
                        'content': script.string[:1000]
                    })
        
        return js_data
    
    def search_for_api_endpoints(self, html_content):
        """æœç´¢å¯èƒ½çš„APIç«¯ç‚¹"""
        print("\nğŸ” æœç´¢APIç«¯ç‚¹...")
        
        # APIç«¯ç‚¹æ¨¡å¼
        api_patterns = [
            r'https?://[^"\'\s]+/api/[^"\'\s]+',
            r'/api/[^"\'\s]+',
            r'https?://[^"\'\s]+\.json[^"\'\s]*',
            r'/catalog/[^"\'\s]+\.json',
            r'/programs[^"\'\s]*\.json',
            r'uwaterloo\.ca/[^"\'\s]+/programs[^"\'\s]*'
        ]
        
        found_endpoints = []
        
        for pattern in api_patterns:
            matches = re.findall(pattern, html_content, re.IGNORECASE)
            found_endpoints.extend(matches)
        
        # å»é‡å¹¶è¿‡æ»¤
        unique_endpoints = list(set(found_endpoints))
        
        print(f"å‘ç° {len(unique_endpoints)} ä¸ªæ½œåœ¨APIç«¯ç‚¹:")
        for endpoint in unique_endpoints:
            print(f"  â€¢ {endpoint}")
        
        return unique_endpoints
    
    def test_discovered_endpoints(self, endpoints):
        """æµ‹è¯•å‘ç°çš„ç«¯ç‚¹"""
        print("\nğŸ§ª æµ‹è¯•å‘ç°çš„ç«¯ç‚¹...")
        
        results = {}
        
        for endpoint in endpoints:
            # æ„é€ å®Œæ•´URL
            if endpoint.startswith('/'):
                full_url = f"https://uwaterloo.ca{endpoint}"
            elif not endpoint.startswith('http'):
                full_url = f"https://uwaterloo.ca/academic-calendar/graduate-studies/{endpoint}"
            else:
                full_url = endpoint
            
            print(f"  æµ‹è¯•: {full_url}")
            
            try:
                response = self.session.get(full_url, timeout=15)
                results[full_url] = {
                    'status_code': response.status_code,
                    'content_length': len(response.text),
                    'content_type': response.headers.get('Content-Type', 'Unknown'),
                    'is_json': response.headers.get('Content-Type', '').startswith('application/json')
                }
                
                if response.status_code == 200:
                    print(f"    âœ… æˆåŠŸ! {len(response.text):,} chars")
                    
                    # å¦‚æœæ˜¯JSONï¼Œå°è¯•è§£æ
                    if results[full_url]['is_json']:
                        try:
                            data = response.json()
                            results[full_url]['json_keys'] = list(data.keys()) if isinstance(data, dict) else 'array'
                            print(f"    ğŸ“„ JSONæ•°æ®åŒ…å«: {results[full_url]['json_keys']}")
                        except:
                            print(f"    âš ï¸ JSONè§£æå¤±è´¥")
                
            except Exception as e:
                results[full_url] = {'error': str(e)}
                print(f"    âŒ å¤±è´¥: {e}")
        
        return results
    
    def save_analysis_results(self, page_analysis, js_data, endpoints, endpoint_results):
        """ä¿å­˜åˆ†æç»“æœ"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"academic_calendar_analysis_{timestamp}.json"
        
        full_analysis = {
            'analysis_timestamp': datetime.now().isoformat(),
            'target_url': self.print_url,
            'page_analysis': page_analysis,
            'javascript_data': js_data,
            'discovered_endpoints': endpoints,
            'endpoint_test_results': endpoint_results
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(full_analysis, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ å®Œæ•´åˆ†æç»“æœä¿å­˜åˆ°: {filename}")
        return filename

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ Academic Calendar ç›´æ¥è®¿é—®åˆ†æå·¥å…·")
    print("="*70)
    print("ç›®æ ‡URL: https://uwaterloo.ca/academic-calendar/graduate-studies/catalog#/programs?isPrint=true")
    
    scraper = AcademicCalendarScraper()
    
    # 1. è®¿é—®ä¸»é¡µé¢
    html_content = scraper.access_academic_calendar()
    if not html_content:
        print("âŒ æ— æ³•è®¿é—®ä¸»é¡µé¢ï¼Œé€€å‡ºåˆ†æ")
        return
    
    # 2. åˆ†æé¡µé¢ç»“æ„
    page_analysis = scraper.analyze_page_structure(html_content)
    
    print(f"\nğŸ“‹ é¡µé¢åˆ†æç»“æœ:")
    print(f"  æ ‡é¢˜: {page_analysis['title']}")
    print(f"  éœ€è¦JavaScript: {page_analysis['has_javascript_loading']}")
    print(f"  Angularåº”ç”¨: {page_analysis['has_angular_app']}")
    print(f"  è„šæœ¬æ•°é‡: {page_analysis['scripts']}")
    
    # 3. æå–JavaScriptæ•°æ®
    js_data = scraper.extract_javascript_data(html_content)
    print(f"\nğŸ’» JavaScriptåˆ†æ:")
    print(f"  å¤–éƒ¨è„šæœ¬: {len(js_data['script_sources'])}")
    print(f"  å†…è”è„šæœ¬: {len(js_data['inline_scripts'])}")
    print(f"  ç¨‹åºç›¸å…³æ•°æ®: {len(js_data['potential_data'])}")
    
    # 4. æœç´¢APIç«¯ç‚¹
    endpoints = scraper.search_for_api_endpoints(html_content)
    
    # 5. å°è¯•æ›¿ä»£URL
    alternative_results = scraper.try_alternative_urls()
    
    # 6. æµ‹è¯•å‘ç°çš„ç«¯ç‚¹
    if endpoints:
        endpoint_results = scraper.test_discovered_endpoints(endpoints)
    else:
        endpoint_results = {}
    
    # 7. ä¿å­˜ç»“æœ
    analysis_file = scraper.save_analysis_results(page_analysis, js_data, endpoints, endpoint_results)
    
    print(f"\nâœ¨ åˆ†æå®Œæˆ!")
    print(f"ğŸ” å‘ç°çš„å…³é”®ä¿¡æ¯:")
    print(f"  â€¢ é¡µé¢ä½¿ç”¨JavaScriptåŠ¨æ€åŠ è½½å†…å®¹")
    print(f"  â€¢ å‘ç° {len(endpoints)} ä¸ªæ½œåœ¨APIç«¯ç‚¹")
    print(f"  â€¢ æµ‹è¯•äº† {len(alternative_results)} ä¸ªæ›¿ä»£URL")
    
    return analysis_file

if __name__ == "__main__":
    main() 